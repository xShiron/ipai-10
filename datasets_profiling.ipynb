{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ydata_profiling\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = pd.read_csv(\"./Datasets/nba_elo.csv\")\n",
    "pollution = pd.read_csv(\"./Datasets/pollution_2000_2023.csv\", index_col=[0])\n",
    "us_accidents = pd.read_csv(\"./Datasets/US_Accidents_March23.csv\")\n",
    "nba_teams = pd.read_csv(\"./Datasets/team.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preserving the original data to not lose any information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_original = nba.copy()\n",
    "pollution_original = pollution.copy()\n",
    "us_accidents_original = us_accidents.copy()\n",
    "nba_teams_original = nba_teams.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction of the US Traffic Accidents dataset to only the important columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_accidents = us_accidents[[\"Severity\", \"Start_Time\", \"City\", \"State\", \n",
    "              \"Amenity\", \"Bump\", \"Crossing\",\"Give_Way\",\n",
    "              \"Junction\",\"No_Exit\",\"Railway\",\"Roundabout\",\n",
    "              \"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\", \"Start_Lat\", \"Start_Lng\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 287/287 [00:31<00:00,  9.12it/s, Completed]                           \n",
      "Generate report structure: 100%|██████████| 1/1 [00:04<00:00,  4.49s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:04<00:00,  4.86s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 98.20it/s]\n"
     ]
    }
   ],
   "source": [
    "profile = ydata_profiling.ProfileReport(nba, title=\"\")\n",
    "profile.to_file(\"Datasets_profiling/nba_profiling.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the data quality in the NBA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(nba.duplicated()).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing string columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize the Address, State and County columns we'll do the following:\n",
    "* Make them all lower case;\n",
    "* Remove all whitespace;\n",
    "* Remove all spaces;\n",
    "* Remove all special characters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making all cities lower case.\n",
    "nba[\"team1\"] = nba[\"team1\"].str.upper()\n",
    "nba[\"team2\"] = nba[\"team2\"].str.upper()\n",
    "\n",
    "# Removal of whitespace from all cities.\n",
    "nba[\"team1\"] = nba[\"team1\"].str.strip()\n",
    "nba[\"team2\"] = nba[\"team2\"].str.strip()\n",
    "\n",
    "# Removal of spaces from cities.\n",
    "nba[\"team1\"] = nba[\"team1\"].str.replace(\" \", \"\")\n",
    "nba[\"team2\"] = nba[\"team2\"].str.replace(\" \", \"\")\n",
    "\n",
    "# Removal of special characters.\n",
    "nba[\"team1\"] = nba[\"team1\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "nba[\"team2\"] = nba[\"team2\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Playoff column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The playoff column has a very strange peculiarity. \\ \n",
    "Before 2016 playoff games are marked as true ('t' in the dataset ) or nothing if they're not playoff games (NaN in the dataset). \\ \n",
    "However, from 2016 onwards the dataset marks the type of playoff game, instead of just using 't'. \\\n",
    "The following characters are used in the dataset to designate:\n",
    "* 'q': Quarterfinal \n",
    "* 's': Semifinal\n",
    "* 'c': Conference final\n",
    "* 'f': Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity's sake we'll consider any playoff match as just a True and the rest as False. \\\n",
    "However, it's worth nothing that it could prove to be interesting to analyze if a team losing a final has a bigger impact than a regular playoff game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t    4033\n",
       "q     144\n",
       "s      43\n",
       "c      22\n",
       "f      12\n",
       "Name: playoff, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba[\"playoff\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba[\"playoff\"] = nba[\"playoff\"].apply(lambda x: True if x == 't' or x == 'q' or x == 's' or x == 'f' or x == 'c' else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    62814\n",
       "True      4254\n",
       "Name: playoff, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba[\"playoff\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Pollution dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 286/286 [01:06<00:00,  4.33it/s, Completed]                                   \n",
      "Generate report structure: 100%|██████████| 1/1 [00:04<00:00,  4.87s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 52.86it/s]\n"
     ]
    }
   ],
   "source": [
    "profile = ydata_profiling.ProfileReport(pollution, title=\"\")\n",
    "profile.to_file(\"Datasets_profiling/air_pollution_profiling.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the data quality in the Air Pollution dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Duplicate rows: This datasate has 1739 duplicate rows (0.3%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop duplicate rows from this dataset since we've the date when it occurred it makes no sense to have duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing String columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize the Address, State and County columns we'll do the following:\n",
    "* Make them all upper case;\n",
    "* Remove all whitespace;\n",
    "* Remove all spaces;\n",
    "* Remove all special characters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making all cities lower case.\n",
    "pollution[\"City\"] = pollution[\"City\"].str.upper()\n",
    "pollution[\"State\"] = pollution[\"State\"].str.upper()\n",
    "pollution[\"County\"] = pollution[\"County\"].str.upper()\n",
    "\n",
    "# Removal of whitespace from all cities.\n",
    "pollution[\"City\"] = pollution[\"City\"].str.strip()\n",
    "pollution[\"State\"] = pollution[\"State\"].str.strip()\n",
    "pollution[\"County\"] = pollution[\"County\"].str.strip()\n",
    "\n",
    "# Removal of spaces from cities.\n",
    "pollution[\"City\"] = pollution[\"City\"].str.replace(\" \", \"\")\n",
    "pollution[\"State\"] = pollution[\"State\"].str.replace(\" \", \"\")\n",
    "pollution[\"County\"] = pollution[\"County\"].str.replace(\" \", \"\")\n",
    "\n",
    "# Removal of special characters.\n",
    "pollution[\"City\"] = pollution[\"City\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "pollution[\"State\"] = pollution[\"State\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "pollution[\"County\"] = pollution[\"County\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning errors in the state column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_stringmatching.similarity_measure.levenshtein as lev\n",
    "\n",
    "us_states = [\n",
    "    \"ALABAMA\", \"ALASKA\", \"ARIZONA\", \"ARKANSAS\", \"CALIFORNIA\", \"COLORADO\",\n",
    "    \"CONNECTICUT\", \"DELAWARE\", \"FLORIDA\", \"GEORGIA\", \"HAWAII\", \"IDAHO\",\n",
    "    \"ILLINOIS\", \"INDIANA\", \"IOWA\", \"KANSAS\", \"KENTUCKY\", \"LOUISIANA\",\n",
    "    \"MAINE\", \"MARYLAND\", \"MASSACHUSETTS\", \"MICHIGAN\", \"MINNESOTA\",\n",
    "    \"MISSISSIPPI\", \"MISSOURI\", \"MONTANA\", \"NEBRASKA\", \"NEVADA\",\n",
    "    \"NEWHAMPSHIRE\", \"NEWJERSEY\", \"NEWMEXICO\", \"NEWYORK\",\n",
    "    \"NORTHCAROLINA\", \"NORTHDAKOTA\", \"OHIO\", \"OKLAHOMA\", \"OREGON\",\n",
    "    \"PENNSYLVANIA\", \"RHODEISLAND\", \"SOUTHCAROLINA\", \"SOUTHDAKOTA\",\n",
    "    \"TENNESSEE\", \"TEXAS\", \"UTAH\", \"VERMONT\", \"VIRGINIA\", \"WASHINGTON\",\n",
    "    \"WESTVIRGINIA\", \"WISCONSIN\", \"WYOMING\"\n",
    "]\n",
    "\n",
    "def clean_state(df, col_name):\n",
    "    leven = lev.Levenshtein()\n",
    "\n",
    "    def best_match(state_str):\n",
    "        max_score = -1\n",
    "        best_state = state_str\n",
    "        for state in us_states:\n",
    "            score = leven.get_sim_score(state_str, state)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_state = state\n",
    "        return best_state\n",
    "\n",
    "    df[col_name] = df[col_name].apply(best_match)\n",
    "\n",
    "clean_state(pollution, \"State\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes about ~3 minutes to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize the numerical columns we'll do the following procedures:\n",
    "* Fill missing values with the median;\n",
    "* Check for values out of bounds;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_median = pollution[\"O3 AQI\"].median()\n",
    "co_median = pollution[\"CO AQI\"].median()\n",
    "so2_median = pollution[\"SO2 AQI\"].median()\n",
    "no2_median = pollution[\"NO2 AQI\"].median()\n",
    "\n",
    "pollution[\"O3 AQI\"].fillna(o3_median, inplace=True)\n",
    "pollution[\"CO AQI\"].fillna(co_median, inplace=True)\n",
    "pollution[\"SO2 AQI\"].fillna(so2_median, inplace=True)\n",
    "pollution[\"NO2 AQI\"].fillna(no2_median, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AQI in the US can only have values between 0-500, therefore we need to look for values out of this range and clean them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution[\"O3 AQI\"] = pollution[\"O3 AQI\"].apply(lambda x: o3_median if x < 0 or x > 500 else x)\n",
    "pollution[\"CO AQI\"] = pollution[\"CO AQI\"].apply(lambda x: co_median if x < 0 or x > 500 else x)\n",
    "pollution[\"SO2 AQI\"] = pollution[\"SO2 AQI\"].apply(lambda x: so2_median if x < 0 or x > 500 else x)\n",
    "pollution[\"NO2 AQI\"] = pollution[\"NO2 AQI\"].apply(lambda x: no2_median if x < 0 or x > 500 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA teams dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 20/20 [00:00<00:00, 32.85it/s, Completed]                         \n",
      "Generate report structure: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 361.80it/s]\n"
     ]
    }
   ],
   "source": [
    "profile = ydata_profiling.ProfileReport(nba_teams, title=\"\")\n",
    "profile.to_file(\"Datasets_profiling/nba_teams_profiling.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the data quality in the NBA teams dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is very small, having only 30 rows of data. Therefore, if it had any errors they could easily be corrected by hand. \\\n",
    "However, we'll define some automatic cleaning routines anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_teams.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows in this dataset, and if there are any duplciated rows inserted it makes no sense to keep them. Since these rows just represent general information about a team, having more than one row for a specific team makes no sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing String columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize the State, Nickname, Abbreviation, City and Full_Name columns we'll do the following:\n",
    "* Make them all lower case;\n",
    "* Remove all whitespace;\n",
    "* Remove all spaces;\n",
    "* Remove all special characters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making all cities lower case.\n",
    "nba_teams[\"city\"] = nba_teams[\"city\"].str.upper()\n",
    "nba_teams[\"state\"] = nba_teams[\"state\"].str.upper()\n",
    "nba_teams[\"nickname\"] = nba_teams[\"nickname\"].str.upper()\n",
    "nba_teams[\"full_name\"] = nba_teams[\"full_name\"].str.upper()\n",
    "\n",
    "# Removal of whitespace from all cities.\n",
    "nba_teams[\"city\"] = nba_teams[\"city\"].str.strip()\n",
    "nba_teams[\"state\"] = nba_teams[\"state\"].str.strip()\n",
    "nba_teams[\"nickname\"] = nba_teams[\"nickname\"].str.strip()\n",
    "nba_teams[\"full_name\"] = nba_teams[\"full_name\"].str.strip()\n",
    "\n",
    "# Removal of spaces from cities.\n",
    "nba_teams[\"city\"] = nba_teams[\"city\"].str.replace(\" \", \"\")\n",
    "nba_teams[\"state\"] = nba_teams[\"state\"].str.replace(\" \", \"\")\n",
    "nba_teams[\"nickname\"] = nba_teams[\"nickname\"].str.replace(\" \", \"\")\n",
    "nba_teams[\"full_name\"] = nba_teams[\"full_name\"].str.replace(\" \", \"\")\n",
    "\n",
    "# Removal of special characters.\n",
    "nba_teams[\"city\"] = nba_teams[\"city\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "nba_teams[\"state\"] = nba_teams[\"state\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "nba_teams[\"nickname\"] = nba_teams[\"nickname\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "nba_teams[\"full_name\"] = nba_teams[\"full_name\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning errors in the state column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find errors in the state column we'll use the Levenshtein similarity measure to find typos in the state names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_stringmatching.similarity_measure.levenshtein as lev\n",
    "\n",
    "us_states = [\n",
    "    \"ALABAMA\", \"ALASKA\", \"ARIZONA\", \"ARKANSAS\", \"CALIFORNIA\", \"COLORADO\",\n",
    "    \"CONNECTICUT\", \"DELAWARE\", \"FLORIDA\", \"GEORGIA\", \"HAWAII\", \"IDAHO\",\n",
    "    \"ILLINOIS\", \"INDIANA\", \"IOWA\", \"KANSAS\", \"KENTUCKY\", \"LOUISIANA\",\n",
    "    \"MAINE\", \"MARYLAND\", \"MASSACHUSETTS\", \"MICHIGAN\", \"MINNESOTA\",\n",
    "    \"MISSISSIPPI\", \"MISSOURI\", \"MONTANA\", \"NEBRASKA\", \"NEVADA\",\n",
    "    \"NEWHAMPSHIRE\", \"NEWJERSEY\", \"NEWMEXICO\", \"NEWYORK\",\n",
    "    \"NORTHCAROLINA\", \"NORTHDAKOTA\", \"OHIO\", \"OKLAHOMA\", \"OREGON\",\n",
    "    \"PENNSYLVANIA\", \"RHODEISLAND\", \"SOUTHCAROLINA\", \"SOUTHDAKOTA\",\n",
    "    \"TENNESSEE\", \"TEXAS\", \"UTAH\", \"VERMONT\", \"VIRGINIA\", \"WASHINGTON\",\n",
    "    \"WESTVIRGINIA\", \"WISCONSIN\", \"WYOMING\"\n",
    "]\n",
    "\n",
    "clean_state(nba_teams, \"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Traffic Accidents dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 31/31 [03:47<00:00,  7.35s/it, Completed]                        \n",
      "Generate report structure: 100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 291.29it/s]\n"
     ]
    }
   ],
   "source": [
    "profile = ydata_profiling.ProfileReport(us_accidents, title=\"\")\n",
    "profile.to_file(\"Datasets_profiling/us_accidents_profiling.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the data quality in the US Traffic Accidents dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Duplicate rows: This dataset has 502007 duplicate rows (6.5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_accidents.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rows get dropped, since they are exact replicas of other rows and repition in this dataset doesn't add any valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* City columns: Has 256 NaN values and some are upper case and lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Amenity</th>\n",
       "      <th>Bump</th>\n",
       "      <th>Crossing</th>\n",
       "      <th>Give_Way</th>\n",
       "      <th>Junction</th>\n",
       "      <th>No_Exit</th>\n",
       "      <th>Railway</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85968</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-08-02 18:18:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>34.451862</td>\n",
       "      <td>-117.660103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111080</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-01 10:26:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>34.451862</td>\n",
       "      <td>-117.660103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119772</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-23 19:30:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>27.388653</td>\n",
       "      <td>-82.441948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122929</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-17 17:34:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27.387951</td>\n",
       "      <td>-82.440239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123702</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-01-24 07:30:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>27.388653</td>\n",
       "      <td>-82.441948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597826</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-12-13 18:38:56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>38.965710</td>\n",
       "      <td>-77.002460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606562</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-11-06 09:08:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>41.036740</td>\n",
       "      <td>-73.675490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7614480</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-11-24 04:48:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IA</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>40.586870</td>\n",
       "      <td>-92.985090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7619724</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-09 15:46:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NY</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>41.036740</td>\n",
       "      <td>-73.675490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7629988</th>\n",
       "      <td>4</td>\n",
       "      <td>2017-09-06 06:51:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>41.153420</td>\n",
       "      <td>-84.803490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Severity           Start_Time City State  Amenity   Bump  Crossing  \\\n",
       "85968           2  2016-08-02 18:18:02  NaN    CA    False  False     False   \n",
       "111080          2  2016-06-01 10:26:08  NaN    CA    False  False     False   \n",
       "119772          2  2016-12-23 19:30:59  NaN    FL    False  False      True   \n",
       "122929          2  2017-01-17 17:34:09  NaN    FL    False  False     False   \n",
       "123702          2  2017-01-24 07:30:44  NaN    FL    False  False      True   \n",
       "...           ...                  ...  ...   ...      ...    ...       ...   \n",
       "7597826         2  2017-12-13 18:38:56  NaN    DC    False  False     False   \n",
       "7606562         2  2017-11-06 09:08:42  NaN    NY    False  False      True   \n",
       "7614480         4  2017-11-24 04:48:21  NaN    IA    False  False     False   \n",
       "7619724         2  2017-10-09 15:46:08  NaN    NY    False  False      True   \n",
       "7629988         4  2017-09-06 06:51:55  NaN    IN    False  False     False   \n",
       "\n",
       "         Give_Way  Junction  No_Exit  Railway  Roundabout  Station   Stop  \\\n",
       "85968       False     False    False    False       False    False  False   \n",
       "111080      False     False    False    False       False    False  False   \n",
       "119772      False     False    False    False       False    False  False   \n",
       "122929      False     False    False    False       False    False  False   \n",
       "123702      False     False    False    False       False    False  False   \n",
       "...           ...       ...      ...      ...         ...      ...    ...   \n",
       "7597826     False     False    False    False       False    False  False   \n",
       "7606562     False      True    False    False       False    False   True   \n",
       "7614480     False     False    False    False       False    False   True   \n",
       "7619724     False      True    False    False       False    False   True   \n",
       "7629988     False     False    False    False       False    False  False   \n",
       "\n",
       "         Traffic_Calming  Traffic_Signal  Start_Lat   Start_Lng  \n",
       "85968              False           False  34.451862 -117.660103  \n",
       "111080             False           False  34.451862 -117.660103  \n",
       "119772             False            True  27.388653  -82.441948  \n",
       "122929             False           False  27.387951  -82.440239  \n",
       "123702             False            True  27.388653  -82.441948  \n",
       "...                  ...             ...        ...         ...  \n",
       "7597826            False            True  38.965710  -77.002460  \n",
       "7606562            False           False  41.036740  -73.675490  \n",
       "7614480            False           False  40.586870  -92.985090  \n",
       "7619724            False           False  41.036740  -73.675490  \n",
       "7629988            False           False  41.153420  -84.803490  \n",
       "\n",
       "[240 rows x 18 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_accidents[us_accidents.City.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling NaN city values using coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing String columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a geolocator api, we can fill more than half of the NaN city values, as for the other NaN values they get a NoCity value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because they happen between cities or villages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geo_lookup\")\n",
    "\n",
    "idx_location_list = list()\n",
    "idx_no_city = list()\n",
    "for index in us_accidents[us_accidents.City.isnull()].index:\n",
    "\n",
    "    # Obtaining the Latitude and the Longitute associated with the row.\n",
    "    latitude = us_accidents.loc[index, \"Start_Lat\"]\n",
    "    longitude = us_accidents.loc[index, \"Start_Lng\"]\n",
    "    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
    "\n",
    "    # Check if it's a city, a locality or a village.\n",
    "    address = location.raw.get('address', {})\n",
    "    city = address.get('city', None)\n",
    "    locality = address.get('locality', None)\n",
    "    village = address.get('village', None)\n",
    "\n",
    "    us_accidents.loc[index, \"City\"] = city if city else locality if locality else village if village else \"NOCITY\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes about ~4-5 minutes to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize the State and City columns we'll do the following:\n",
    "* Make them all lower case;\n",
    "* Remove all whitespace;\n",
    "* Remove all spaces;\n",
    "* Remove all special characters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making all cities lower case.\n",
    "us_accidents[\"City\"] = us_accidents[\"City\"].str.upper()\n",
    "us_accidents[\"State\"] = us_accidents[\"State\"].str.upper()\n",
    "\n",
    "# Removal of whitespace from all cities.\n",
    "us_accidents[\"City\"] = us_accidents[\"City\"].str.strip()\n",
    "us_accidents[\"State\"] = us_accidents[\"State\"].str.strip()\n",
    "\n",
    "# Removal of spaces from cities.\n",
    "us_accidents[\"City\"] = us_accidents[\"City\"].str.replace(\" \", \"\")\n",
    "us_accidents[\"State\"] = us_accidents[\"State\"].str.replace(\" \", \"\")\n",
    "\n",
    "# Removal of special characters.\n",
    "us_accidents[\"City\"] = us_accidents[\"City\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "us_accidents[\"State\"] = us_accidents[\"State\"].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes about ~30 seconds to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning errors in the State column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Jaro similarity measure we compute the differences between state abbreviations to correct typos.\n",
    "\n",
    "The Jaro measure is a type of edit distance, developed mainly to compare short strings, such as first and last names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_stringmatching.similarity_measure.jaro as jaro\n",
    "\n",
    "us_state_abbreviations = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "]\n",
    "\n",
    "def clean_state_abbreviations(df, col_name):\n",
    "    leven = lev.Levenshtein()\n",
    "\n",
    "    def best_match(state_str):\n",
    "        max_score = -1\n",
    "        best_state = state_str\n",
    "        for state in us_state_abbreviations:\n",
    "            score = leven.get_sim_score(state_str, state)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_state = state\n",
    "        return best_state\n",
    "\n",
    "    df[col_name] = df[col_name].apply(best_match)\n",
    "\n",
    "clean_state_abbreviations(us_accidents, \"State\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes about ~16 minutes to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
